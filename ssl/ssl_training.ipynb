{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset_mnist import *\n",
    "from augmentations import *\n",
    "from cnn_model     import *\n",
    "from umap_projection import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset\n",
    "\n",
    "sample random batch x, y = dataset.get_batch(batch_size),\n",
    "\n",
    "x input with shape (batch_size, 1, 28, 28)\n",
    "\n",
    "y labels with shape (batch_size, ) (not used)\n",
    "\n",
    "execute two sets of augmentation and plot augmented example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## augmentations\n",
    "\n",
    "aditive gaussian noise\n",
    "\n",
    "random color inversion (negative)\n",
    "\n",
    "random pixel zeroing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  torch.Size([5, 1, 28, 28]) tensor(0.1223) tensor(0.2994) tensor(1.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DatasetMnist()\n",
    "batch_size = 5 # 256    \n",
    "\n",
    "x, y = dataset.get_batch(batch_size)\n",
    "\n",
    "print(\"x = \", x.shape, x.mean(), x.std(), x.max())\n",
    "\n",
    "x_orig   = x\n",
    "    \n",
    "#add noise\n",
    "x_noised = aug_noise(x_orig)\n",
    "\n",
    "plt.imshow(x[0][0].detach().cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(x_noised[0][0].detach().cpu().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vicreg loss - non contrastive self supervised loss\n",
    "\n",
    "- similarity, sim_loss : makes same inputs close\n",
    "- variance, std_za, std_za : batch-wise variance, prevent collapsing, forces model to generate unique features for every different input\n",
    "- covariance, cov_za, cov_zb : element-wise (feature-wise) decorrelation, forces model to generate unique features - output from model within single input are not correlate, a.k.a : each features output (each of 256 signals) gives unique infomation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simclr_loss_func(za, zb):\n",
    "    batch_size = za.shape[0]    \n",
    "\n",
    "    # create the similarity matrix\n",
    "    sim_matrix = torch.mm(za, zb.T)/za.shape[-1]\n",
    "\n",
    "    target = torch.zeros(batch_size, batch_size)\n",
    "    target[0:batch_size//2, 0:batch_size//2] = 1.0\n",
    "    target[batch_size//2:, batch_size//2:] = 1.0\n",
    "\n",
    "    print(target)\n",
    "\n",
    "    loss = ((target - sim_matrix)**2).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _off_diagonal(x):\n",
    "    mask = 1.0 - torch.eye(x.shape[0], device=x.device)\n",
    "    return x*mask \n",
    "\n",
    "def vicreg_loss_func(za, zb): \n",
    "    # invariance loss\n",
    "    sim_loss = ((za - zb)**2).mean()\n",
    "\n",
    "    # variance loss\n",
    "    std_za = torch.sqrt(za.var(dim=0) + 1e-6)\n",
    "    std_zb = torch.sqrt(zb.var(dim=0) + 1e-6) \n",
    "    \n",
    "    std_loss = torch.mean(torch.relu(1.0 - std_za)) \n",
    "    std_loss+= torch.mean(torch.relu(1.0 - std_zb))\n",
    "   \n",
    "    # covariance loss \n",
    "    za_norm = za - za.mean(dim=0)\n",
    "    zb_norm = zb - zb.mean(dim=0)\n",
    "    cov_za = (za_norm.T @ za_norm) / (za.shape[0] - 1.0)\n",
    "    cov_zb = (zb_norm.T @ zb_norm) / (zb.shape[0] - 1.0)\n",
    "    \n",
    "    cov_loss = _off_diagonal(cov_za).pow_(2).sum()/za.shape[1] \n",
    "    cov_loss+= _off_diagonal(cov_zb).pow_(2).sum()/zb.shape[1]\n",
    "\n",
    "    # total vicreg loss\n",
    "    loss = 1.0*sim_loss + 1.0*std_loss + (1.0/25.0)*cov_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dist_loss_func(za, zb, k = 5): \n",
    "    # similarity loss\n",
    "    similarity_matrix = torch.nn.functional.cosine_similarity(za.unsqueeze(1), zb.unsqueeze(0), dim=-1)\n",
    "\n",
    "    _, indices = similarity_matrix.topk(k, dim=-1)\n",
    "\n",
    "    batch_size = za.shape[0]\n",
    "\n",
    "    similarity_target = torch.zeros_like(similarity_matrix)\n",
    "    similarity_target[torch.arange(batch_size).unsqueeze(1), indices] = 1.0\n",
    "\n",
    "    sim_loss = ((similarity_target - similarity_matrix)**2).mean()\n",
    "    sim_loss = sim_loss.mean()\n",
    "    \n",
    "    \n",
    "\n",
    "    # variance loss\n",
    "    std_za = torch.sqrt(za.var(dim=0) + 1e-6)\n",
    "    std_zb = torch.sqrt(zb.var(dim=0) + 1e-6) \n",
    "    \n",
    "    std_loss = torch.mean(torch.relu(1.0 - std_za)) \n",
    "    std_loss+= torch.mean(torch.relu(1.0 - std_zb))\n",
    "   \n",
    "    # covariance loss \n",
    "    za_norm = za - za.mean(dim=0)\n",
    "    zb_norm = zb - zb.mean(dim=0)\n",
    "    cov_za = (za_norm.T @ za_norm) / (za.shape[0] - 1.0)\n",
    "    cov_zb = (zb_norm.T @ zb_norm) / (zb.shape[0] - 1.0)\n",
    "    \n",
    "    cov_loss = _off_diagonal(cov_za).pow_(2).sum()/za.shape[1] \n",
    "    cov_loss+= _off_diagonal(cov_zb).pow_(2).sum()/zb.shape[1]\n",
    "\n",
    "    # total vicreg loss\n",
    "    loss = 1.0*sim_loss + 1.0*std_loss + (1.0/25.0)*cov_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "starting training with loss  <function simclr_loss_func at 0x102e88820>\n",
      "Model(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): SiLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): SiLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): SiLU()\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): SiLU()\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=6272, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "0 12000 tensor(0.5000, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "1 12000 tensor(0.5000, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "2 12000 tensor(0.5000, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "3 12000 tensor(0.5000, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "4 12000 tensor(0.4997, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "5 12000 tensor(0.4981, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "6 12000 tensor(0.4874, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "7 12000 tensor(0.4562, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "8 12000 tensor(0.3776, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "9 12000 tensor(0.2986, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "10 12000 tensor(0.2927, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "11 12000 tensor(0.2995, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "12 12000 tensor(0.2892, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "13 12000 tensor(0.2792, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "14 12000 tensor(0.2727, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "15 12000 tensor(0.2755, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "16 12000 tensor(0.2587, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "17 12000 tensor(0.2705, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "18 12000 tensor(0.2827, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "19 12000 tensor(0.2650, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "20 12000 tensor(0.2688, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "21 12000 tensor(0.2901, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "22 12000 tensor(0.2879, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "23 12000 tensor(0.2675, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "24 12000 tensor(0.2897, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "25 12000 tensor(0.2532, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "26 12000 tensor(0.2551, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "27 12000 tensor(0.2546, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "28 12000 tensor(0.2561, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "29 12000 tensor(0.2581, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "30 12000 tensor(0.2605, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "31 12000 tensor(0.2531, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "32 12000 tensor(0.2559, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "33 12000 tensor(0.2581, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "34 12000 tensor(0.2524, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "35 12000 tensor(0.2527, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "36 12000 tensor(0.2547, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "37 12000 tensor(0.2541, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "38 12000 tensor(0.2518, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "39 12000 tensor(0.2519, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "40 12000 tensor(0.2526, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "41 12000 tensor(0.2543, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "42 12000 tensor(0.2512, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "43 12000 tensor(0.2554, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "44 12000 tensor(0.2524, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "45 12000 tensor(0.2522, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "46 12000 tensor(0.2516, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "47 12000 tensor(0.2506, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "48 12000 tensor(0.2514, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "49 12000 tensor(0.2512, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "50 12000 tensor(0.2516, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "51 12000 tensor(0.2514, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "52 12000 tensor(0.2521, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "53 12000 tensor(0.2516, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "54 12000 tensor(0.2509, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "55 12000 tensor(0.2509, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "56 12000 tensor(0.2511, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "57 12000 tensor(0.2511, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "58 12000 tensor(0.2523, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "59 12000 tensor(0.2519, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "60 12000 tensor(0.2514, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "61 12000 tensor(0.2522, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "62 12000 tensor(0.2504, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "63 12000 tensor(0.2510, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "64 12000 tensor(0.2521, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "65 12000 tensor(0.2516, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "66 12000 tensor(0.2518, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "67 12000 tensor(0.2523, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "68 12000 tensor(0.2510, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "69 12000 tensor(0.2510, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "70 12000 tensor(0.2509, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "71 12000 tensor(0.2516, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "72 12000 tensor(0.2503, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "73 12000 tensor(0.2509, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "74 12000 tensor(0.2533, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "75 12000 tensor(0.2505, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "76 12000 tensor(0.2513, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "77 12000 tensor(0.2517, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "78 12000 tensor(0.2501, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "79 12000 tensor(0.2504, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "80 12000 tensor(0.2512, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "81 12000 tensor(0.2504, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "82 12000 tensor(0.2507, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "83 12000 tensor(0.2503, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "84 12000 tensor(0.2505, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "85 12000 tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "86 12000 tensor(0.2505, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "87 12000 tensor(0.2503, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "88 12000 tensor(0.2503, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "89 12000 tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "90 12000 tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "91 12000 tensor(0.2507, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "92 12000 tensor(0.2506, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "93 12000 tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "94 12000 tensor(0.2513, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "95 12000 tensor(0.2506, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "96 12000 tensor(0.2505, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "97 12000 tensor(0.2505, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "98 12000 tensor(0.2504, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "99 12000 tensor(0.2504, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "100 12000 tensor(0.2510, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "101 12000 tensor(0.2504, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "102 12000 tensor(0.2503, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "103 12000 tensor(0.2514, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "104 12000 tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "105 12000 tensor(0.2506, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "106 12000 tensor(0.2512, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "107 12000 tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "108 12000 tensor(0.2502, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "109 12000 tensor(0.2505, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n",
      "110 12000 tensor(0.2503, grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_functions = [simclr_loss_func] #, vicreg_loss_func]\n",
    "\n",
    "n_features = 128    \n",
    "\n",
    "\n",
    "# train by all loss functions in lsit\n",
    "for loss_func in loss_functions:\n",
    "    # create model and optimizer\n",
    "    model     = Model(dataset.input_shape, n_features)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"starting training with loss \", loss_func)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    steps_count = len(dataset)//batch_size\n",
    "    for i in range(steps_count):\n",
    "\n",
    "        #get random batch\n",
    "        xa, _      = dataset.get_batch(batch_size)\n",
    "        xa_noised  = aug_noise(xa)\n",
    "\n",
    "        xb, _      = dataset.get_batch(batch_size)\n",
    "        xb_noised  = aug_noise(xb)\n",
    "\n",
    "        xa = torch.concatenate([xa, xa_noised], dim=0)\n",
    "        xb = torch.concatenate([xb, xb_noised], dim=0)\n",
    "\n",
    "        #obtain features\n",
    "        za = model(xa)\n",
    "        zb = model(xb)\n",
    "        \n",
    "        #compute loss\n",
    "        loss = loss_func(za, zb)\n",
    "\n",
    "        #optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(i, steps_count, loss) \n",
    "\n",
    "    result_name = str(loss_func.__name__)\n",
    "    torch.save(model.state_dict(), \"trained/\" + result_name + \".pt\")\n",
    "\n",
    "\n",
    "print(\"training done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features projection\n",
    "\n",
    "2D visualisation of trained feature space\n",
    "\n",
    "sample random batch\n",
    "use UMAP and T-SNE to project 128D space into 2D\n",
    "plot it, with coloring by class ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): SiLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): SiLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): SiLU()\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): SiLU()\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=6272, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michal/Library/Python/3.9/lib/python/site-packages/umap/umap_.py:2437: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michal/Library/Python/3.9/lib/python/site-packages/umap/umap_.py:2437: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m umap_projection(x, y)\n\u001b[1;32m     31\u001b[0m umap_projection(z, y)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtsne_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m tsne_projection(z, y)\n",
      "File \u001b[0;32m~/projects/ai_tutorials/ssl/umap_projection.py:26\u001b[0m, in \u001b[0;36mtsne_projection\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtsne_projection\u001b[39m(x, y):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Fit and transform the data to reduce to 2 dimensions\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     z_proj \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     colormap \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmagenta\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpurple\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrown\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39mscatter(z_proj[:, \u001b[38;5;241m0\u001b[39m], z_proj[:, \u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39mcolormap[y])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/manifold/_t_sne.py:1175\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m-> 1175\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/manifold/_t_sne.py:864\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 864\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "\n",
    "result_name = str(loss_functions[0].__name__)\n",
    "\n",
    "model = Model(dataset.input_shape, n_features)\n",
    "\n",
    "model.load_state_dict(torch.load(\"trained/\" + result_name + \".pt\", map_location = \"cpu\"))\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "x, y      = dataset.get_batch(batch_size)\n",
    "x_noised  = aug_noise(x.clone())\n",
    "\n",
    "x = torch.concatenate([x, x_noised], dim=0)\n",
    "y = torch.concatenate([torch.zeros((batch_size, )), torch.ones((batch_size, ))], dim=0)\n",
    "\n",
    "\n",
    "y = y.detach().cpu().numpy()\n",
    "y = numpy.array(y, dtype=int)\n",
    "\n",
    "\n",
    "z = model(x) \n",
    "dist = torch.cdist(z, z)\n",
    "z = z.detach().cpu().numpy()\n",
    "\n",
    "x = x.flatten(1).detach().cpu().numpy()\n",
    "\n",
    "plt.matshow(dist.detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "umap_projection(x, y)\n",
    "umap_projection(z, y)\n",
    "tsne_projection(x, y)\n",
    "tsne_projection(z, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
