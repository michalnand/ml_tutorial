{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robot environment\n",
    "\n",
    "- balence inverted pendulum by moving cart\n",
    "- state   : 4 values : positions + velocities\n",
    "- actions : 2 discrete : move cart left, move cart right\n",
    "\n",
    "![image](../doc/images/cart_pole.png)\n",
    "![image](../doc/images/diagrams-rl_principle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parallel environments\n",
    "\n",
    "- on-policy agents requires multiple parallel environments to reduce data correlation\n",
    "- we create lis of environments\n",
    "\n",
    "![image](../doc/images/diagrams-envs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import time\n",
    "import gymnasium as gym\n",
    "\n",
    "'''\n",
    "    multiple environments wrapper\n",
    "'''\n",
    "class EnvsList:\n",
    "    def __init__(self, env_name, n_envs, render_mode = None, Wrapper = None, max_steps = 1000):\n",
    "        \n",
    "        self.envs   = []\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        self.steps     = numpy.zeros(n_envs)\n",
    "\n",
    "        for _ in range(n_envs):\n",
    "            if isinstance(env_name, str):\n",
    "                env = gym.make(env_name, render_mode=render_mode)\n",
    "            else:\n",
    "                env = env_name(render_mode=render_mode)\n",
    "\n",
    "            if Wrapper is not None:\n",
    "                env = Wrapper(env)\n",
    "\n",
    "            self.envs.append(env)\n",
    "\n",
    "        self.observation_space = self.envs[0].observation_space\n",
    "        self.action_space      = self.envs[0].action_space\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.envs)\n",
    "\n",
    "    def step(self, actions):\n",
    "\n",
    "        states  = []\n",
    "        rewards = []\n",
    "        dones   = [] \n",
    "        infos   = []\n",
    "\n",
    "        self.steps+= 1\n",
    "        for i in range(len(self.envs)):\n",
    "            state, reward, done, _, info = self.envs[i].step(actions[i])\n",
    "\n",
    "            if self.steps[i] > self.max_steps:\n",
    "                done = True\n",
    "\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            infos.append(info)\n",
    "\n",
    "        states  = numpy.stack(states)\n",
    "        rewards = numpy.stack(rewards)\n",
    "        dones   = numpy.stack(dones)\n",
    "\n",
    "        return states, rewards, dones, infos\n",
    "    \n",
    "    def reset_all(self):\n",
    "        states = []\n",
    "        infos  = []\n",
    "\n",
    "        for i in range(len(self.envs)):\n",
    "            state, info = self.envs[i].reset()\n",
    "           \n",
    "            states.append(state)\n",
    "            infos.append(info)\n",
    "\n",
    "        states  = numpy.stack(states)\n",
    "        self.steps[:] = 0\n",
    "\n",
    "        return states, infos\n",
    "    \n",
    "    def reset(self, env_id):\n",
    "        self.steps[env_id] = 0 \n",
    "        return self.envs[env_id].reset()\n",
    "        \n",
    "    def render(self, env_id):\n",
    "        return self.envs[env_id].render()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.envs[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating envs\n",
      "states shape  (32, 4)\n"
     ]
    }
   ],
   "source": [
    "# create mutliple environments\n",
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "print(\"creating envs\")\n",
    "envs = EnvsList(env_name, 32)\n",
    "states, _ = envs.reset_all()\n",
    "\n",
    "print(\"states shape \", states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO agent\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class AgentPPO():\n",
    "    def __init__(self, envs, Model):\n",
    "        self.envs = envs\n",
    "\n",
    "        # auto select device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # agent hyperparameters\n",
    "        self.gamma              = 0.99\n",
    "        self.entropy_beta       = 0.001\n",
    "        self.eps_clip           = 0.1 \n",
    "        self.adv_coeff          = 1.0\n",
    "        self.val_coeff          = 0.5\n",
    "\n",
    "        self.trajectory_steps   = 128\n",
    "        self.batch_size         = 256\n",
    "        \n",
    "        self.training_epochs    = 4\n",
    "        self.envs_count         = len(envs)\n",
    "        self.learning_rate      = 0.0001\n",
    "\n",
    "\n",
    "        self.state_shape    = self.envs.observation_space.shape\n",
    "        self.actions_count  = self.envs.action_space.n\n",
    "\n",
    "        # policy buffer for storing trajectory\n",
    "        self._buffer_init()\n",
    "        \n",
    "        # create model\n",
    "        self.model = Model(self.state_shape, self.actions_count)\n",
    "        self.model.to(self.device)\n",
    "        print(self.model)\n",
    "\n",
    "        # initialise optimizer and trajectory buffer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "   \n",
    "    # agent main step\n",
    "    def step(self, states, training_enabled = False):        \n",
    "        states_t = torch.tensor(states, dtype=torch.float).to(self.device)\n",
    "\n",
    "        # obtain model output, logits and values\n",
    "        logits_t, values_t  = self.model.forward(states_t)\n",
    "\n",
    "        # sample action, probs computed from logits\n",
    "        action_probs_t        = torch.nn.functional.softmax(logits_t, dim = 1)\n",
    "        action_distribution_t = torch.distributions.Categorical(action_probs_t)\n",
    "        action_t              = action_distribution_t.sample()\n",
    "        actions               = action_t.detach().to(\"cpu\").numpy()\n",
    "       \n",
    "        # environment step\n",
    "        states_new, rewards, dones, infos = self.envs.step(actions)\n",
    "\n",
    "        #put into trajectory buffer\n",
    "        if training_enabled:\n",
    "            self._buffer_add(states_t, logits_t, values_t, actions, rewards, dones)\n",
    "\n",
    "            # if buffer is full, run training loop and clear buffer after\n",
    "            if self.buffer_ptr >= self.trajectory_steps:\n",
    "                self._compute_returns(self.gamma)\n",
    "                self._train()\n",
    "                self._buffer_init()\n",
    "  \n",
    "\n",
    "        return states_new, rewards, dones, infos\n",
    "    \n",
    "    def save(self, result_path):\n",
    "        torch.save(self.model.state_dict(), result_path + \"/model.pt\")\n",
    "\n",
    "    def load(self, result_path):\n",
    "        self.model.load_state_dict(torch.load(result_path + \"/model.pt\", map_location = self.device))\n",
    "\n",
    "    def _train(self): \n",
    "        samples_count = self.trajectory_steps*self.envs_count\n",
    "        batch_count = samples_count//self.batch_size\n",
    "\n",
    "        # epoch training\n",
    "        for e in range(self.training_epochs):\n",
    "            for batch_idx in range(batch_count):\n",
    "                # sample batch\n",
    "                states, logits, actions, returns, advantages = self._sample_batch(self.batch_size)\n",
    "                \n",
    "                # compute main PPO loss\n",
    "                loss_ppo = self.loss_ppo(states, logits, actions, returns, advantages)\n",
    "\n",
    "                self.optimizer.zero_grad()        \n",
    "                loss_ppo.backward()\n",
    "\n",
    "                # gradient clip for stabilising training\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "                self.optimizer.step() \n",
    "\n",
    "         \n",
    "\n",
    "    '''\n",
    "        main PPO loss\n",
    "    '''\n",
    "    def loss_ppo(self, states, logits, actions, returns, advantages):\n",
    "        logits_new, values_new  = self.model.forward(states)\n",
    "\n",
    "        \n",
    "        log_probs_old = torch.nn.functional.log_softmax(logits, dim = 1).detach()\n",
    "\n",
    "        probs_new     = torch.nn.functional.softmax(logits_new,     dim = 1)\n",
    "        log_probs_new = torch.nn.functional.log_softmax(logits_new, dim = 1)\n",
    "\n",
    "        '''\n",
    "            compute critic loss, as MSE\n",
    "            L = (T - V(s))^2\n",
    "        '''\n",
    "        values_new = values_new.squeeze(1)\n",
    "        loss_value = (returns.detach() - values_new)**2\n",
    "        loss_value = loss_value.mean()\n",
    "\n",
    "        ''' \n",
    "            compute actor loss, surrogate loss\n",
    "        '''\n",
    "        advantages       = self.adv_coeff*advantages.detach() \n",
    "        advantages  = (advantages - torch.mean(advantages))/(torch.std(advantages) + 1e-10)\n",
    "\n",
    "        log_probs_new_  = log_probs_new[range(len(log_probs_new)), actions]\n",
    "        log_probs_old_  = log_probs_old[range(len(log_probs_old)), actions]\n",
    "                        \n",
    "        ratio       = torch.exp(log_probs_new_ - log_probs_old_)\n",
    "        p1          = ratio*advantages\n",
    "        p2          = torch.clamp(ratio, 1.0 - self.eps_clip, 1.0 + self.eps_clip)*advantages\n",
    "        loss_policy = -torch.min(p1, p2)  \n",
    "        loss_policy = loss_policy.mean()  \n",
    "    \n",
    "        '''\n",
    "            compute entropy loss, to avoid greedy strategy\n",
    "            L = beta*H(pi(s)) = beta*pi(s)*log(pi(s))\n",
    "        '''\n",
    "        loss_entropy = (probs_new*log_probs_new).sum(dim = 1)\n",
    "        loss_entropy = self.entropy_beta*loss_entropy.mean()\n",
    "\n",
    "        loss = self.val_coeff*loss_value + loss_policy + loss_entropy\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    '''\n",
    "        trajectory buffer methods\n",
    "        this is mostly held in separated class\n",
    "    '''\n",
    "    # trajectory buffer init\n",
    "    def _buffer_init(self):\n",
    "        self.states     = torch.zeros((self.trajectory_steps, self.envs_count, ) + self.state_shape, dtype=torch.float32, device=self.device)\n",
    "        self.logits     = torch.zeros((self.trajectory_steps, self.envs_count,  self.actions_count), dtype=torch.float32, device=self.device)\n",
    "        self.values     = torch.zeros((self.trajectory_steps, self.envs_count, ), dtype=torch.float32, device=self.device)        \n",
    "        self.actions    = torch.zeros((self.trajectory_steps, self.envs_count, ), dtype=int, device=self.device)\n",
    "        self.reward     = torch.zeros((self.trajectory_steps, self.envs_count, ), dtype=torch.float32, device=self.device)\n",
    "        self.dones      = torch.zeros((self.trajectory_steps, self.envs_count, ), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.buffer_ptr = 0  \n",
    "\n",
    "    # add new items into buffer\n",
    "    def _buffer_add(self, states, logits, values, actions, rewards, dones):\n",
    "        self.states[self.buffer_ptr]    = states.detach().to(\"cpu\").clone() \n",
    "        self.logits[self.buffer_ptr]    = logits.detach().to(\"cpu\").clone() \n",
    "        self.values[self.buffer_ptr]    = values.squeeze(1).detach().to(\"cpu\").clone() \n",
    "        self.actions[self.buffer_ptr]   = torch.from_numpy(actions)\n",
    "        \n",
    "        self.reward[self.buffer_ptr]    = torch.from_numpy(rewards)\n",
    "        self.dones[self.buffer_ptr]     = torch.from_numpy(dones).float()\n",
    "        \n",
    "        self.buffer_ptr = self.buffer_ptr + 1 \n",
    "\n",
    "\n",
    "    def _compute_returns(self, gamma, lam = 0.95):\n",
    "        self.returns, self.advantages   = self._gae(self.reward, self.values, self.dones, gamma, lam)\n",
    "        \n",
    "        #reshape buffer for faster batch sampling\n",
    "        self.states     = self.states.reshape((self.trajectory_steps*self.envs_count, ) + self.state_shape)\n",
    "        self.logits     = self.logits.reshape((self.trajectory_steps*self.envs_count, self.actions_count))\n",
    "\n",
    "        self.values     = self.values.reshape((self.trajectory_steps*self.envs_count, ))        \n",
    "     \n",
    "        self.actions    = self.actions.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "        \n",
    "        self.reward     = self.reward.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "      \n",
    "        self.dones      = self.dones.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "\n",
    "        self.returns    = self.returns.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "        self.advantages = self.advantages.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "   \n",
    "    # sampel random batch from buffer\n",
    "    def _sample_batch(self, batch_size):\n",
    "        indices         = torch.randint(0, self.envs_count*self.trajectory_steps, size=(batch_size, ))\n",
    "\n",
    "        states          = self.states[indices]\n",
    "        logits          = self.logits[indices]\n",
    "        \n",
    "        actions         = self.actions[indices]\n",
    "        \n",
    "        returns         = self.returns[indices]\n",
    "        advantages      = self.advantages[indices]\n",
    "       \n",
    "        return states, logits, actions, returns, advantages\n",
    "    \n",
    "    # gae returns computing - more stable than basic returns computatiom\n",
    "    def _gae(self, rewards, values, dones, gamma, lam):\n",
    "        buffer_size = rewards.shape[0]\n",
    "        envs_count  = rewards.shape[1]\n",
    "        \n",
    "        returns     = torch.zeros((buffer_size, envs_count), dtype=torch.float32)\n",
    "        advantages  = torch.zeros((buffer_size, envs_count), dtype=torch.float32)\n",
    "\n",
    "        last_gae    = torch.zeros((envs_count), dtype=torch.float32)\n",
    "        \n",
    "        for n in reversed(range(buffer_size-1)):\n",
    "            delta           = rewards[n] + gamma*values[n+1]*(1.0 - dones[n]) - values[n]\n",
    "            last_gae        = delta + gamma*lam*last_gae*(1.0 - dones[n])\n",
    "            \n",
    "            returns[n]      = last_gae + values[n]\n",
    "            advantages[n]   = last_gae\n",
    " \n",
    "        return returns, advantages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor critic model\n",
    "\n",
    "- simple fully connected model\n",
    "- 128 hidden units in two layers\n",
    "- two separated heads for actor anc critic\n",
    "\n",
    "![image](../doc/images/diagrams-fc_model.png)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    two hidden layers FC model for actor critic architecture\n",
    "'''\n",
    "class ModelFC(torch.nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, n_hidden = 128):\n",
    "        super(ModelFC, self).__init__()\n",
    "\n",
    "        n_inputs = input_shape[0]\n",
    "\n",
    "        # FC model, with two hidden layers and two output heads\n",
    "        self.lin0 = torch.nn.Linear(n_inputs, n_hidden)\n",
    "        self.act0 = torch.nn.SiLU()\n",
    "        self.lin1 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.act1 = torch.nn.SiLU()\n",
    "\n",
    "        self.lin_actor  = torch.nn.Linear(n_hidden, n_actions)\n",
    "        self.lin_critic = torch.nn.Linear(n_hidden, 1)\n",
    "\n",
    "        # orthogonal weight init\n",
    "        torch.nn.init.orthogonal_(self.lin0.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.lin0.bias)\n",
    "        torch.nn.init.orthogonal_(self.lin1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        # output layers with lower init gain\n",
    "        torch.nn.init.orthogonal_(self.lin_actor.weight, 0.01)\n",
    "        torch.nn.init.zeros_(self.lin_actor.bias)\n",
    "        torch.nn.init.orthogonal_(self.lin_critic.weight, 0.1)\n",
    "        torch.nn.init.zeros_(self.lin_critic.bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # obtain features\n",
    "        z = self.lin0(state)\n",
    "        z = self.act0(z)\n",
    "        z = self.lin1(z)\n",
    "        z = self.act1(z)\n",
    "\n",
    "        # obtain actor and critic outputs\n",
    "        logits = self.lin_actor(z)\n",
    "        value  = self.lin_critic(z)\n",
    "\n",
    "        return logits, value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelFC(\n",
      "  (lin0): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (act0): SiLU()\n",
      "  (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (act1): SiLU()\n",
      "  (lin_actor): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (lin_critic): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "0 0.0 0.0 0.0\n",
      "1000 43.59 20.062 10.04\n",
      "2000 87.19 21.156 7.754\n",
      "3000 131.97 23.75 13.679\n",
      "4000 175.53 21.5 11.264\n",
      "5000 212.03 33.75 15.067\n",
      "6000 236.88 54.781 36.038\n",
      "7000 254.59 65.531 33.213\n",
      "8000 267.47 89.719 44.32\n",
      "9000 277.69 106.406 52.544\n",
      "10000 284.69 154.531 105.741\n",
      "11000 289.91 279.219 229.223\n",
      "12000 295.62 195.75 142.813\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = AgentPPO(envs, ModelFC)\n",
    "\n",
    "\n",
    "episodes_count  = numpy.zeros(len(envs))\n",
    "rewards_sum     = numpy.zeros(len(envs))\n",
    "rewards_episode = numpy.zeros(len(envs))\n",
    "\n",
    "n_steps = 100000\n",
    "\n",
    "\n",
    "states, _ = envs.reset_all()\n",
    "for n in range(n_steps):\n",
    "    # agent main step\n",
    "    states_new, rewards, dones, infos = agent.step(states, True)\n",
    "\n",
    "    # accumulate rewards for stats\n",
    "    rewards_sum+= rewards\n",
    "\n",
    "    # reset environments which finished episode \n",
    "    dones_idx = numpy.where(dones)[0]\n",
    "    for i in dones_idx:\n",
    "        states_new[i], _ = envs.reset(i)\n",
    "\n",
    "        episodes_count[i]+= 1\n",
    "        rewards_episode[i] = rewards_sum[i]\n",
    "        rewards_sum[i] = 0\n",
    "\n",
    "    states = states_new.copy()\n",
    "\n",
    "    if n%1000 == 0:\n",
    "        episodes_mean = round(episodes_count.mean(), 2)\n",
    "        rewards_mean  = round(rewards_episode.mean(), 3)\n",
    "        rewards_std   = round(rewards_episode.std(), 3)\n",
    "        print(n, episodes_mean, rewards_mean, rewards_std)\n",
    "\n",
    "\n",
    "agent.save(\"CartPole/\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"training done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelFC(\n",
      "  (lin0): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (act0): SiLU()\n",
      "  (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (act1): SiLU()\n",
      "  (lin_actor): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (lin_critic): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 15:53:24.107 Python[47759:47133227] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    }
   ],
   "source": [
    "# create single env\n",
    "envs = EnvsList(env_name, 1, render_mode=\"human\")\n",
    "\n",
    "agent = AgentPPO(envs, ModelFC)\n",
    "agent.load(\"CartPole/\") \n",
    "\n",
    "states, _ = envs.reset_all()\n",
    "n_steps = 1000000\n",
    "for n in range(n_steps):\n",
    "    # agent main step\n",
    "    states_new, rewards, dones, infos = agent.step(states)\n",
    "\n",
    "    # reset environments which finished episode \n",
    "    dones_idx = numpy.where(dones)[0]\n",
    "    for i in dones_idx:\n",
    "        states_new[i], _ = envs.reset(i)\n",
    "\n",
    "    states = states_new.copy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
