{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robot environment\n",
    "\n",
    "- balence inverted pendulum by moving cart\n",
    "- state   : 4 values : positions + velocities\n",
    "- actions : 2 discrete : move cart left, move cart right\n",
    "\n",
    "![image](../doc/images/cart_pole.png)\n",
    "![image](../doc/images/diagrams-rl_principle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parallel environments\n",
    "\n",
    "- on-policy agents requires multiple parallel environments to reduce data correlation\n",
    "- we create lis of environments\n",
    "\n",
    "![image](../doc/images/diagrams-envs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.9.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import time\n",
    "import gymnasium as gym\n",
    "\n",
    "'''\n",
    "    multiple environments wrapper\n",
    "'''\n",
    "class EnvsList:\n",
    "    def __init__(self, env_name, n_envs, render_mode = None, Wrapper = None, max_steps = 1000):\n",
    "        \n",
    "        self.envs   = []\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        self.steps     = numpy.zeros(n_envs)\n",
    "\n",
    "        for _ in range(n_envs):\n",
    "            if isinstance(env_name, str):\n",
    "                env = gym.make(env_name, render_mode=render_mode)\n",
    "            else:\n",
    "                env = env_name(render_mode=render_mode)\n",
    "\n",
    "            if Wrapper is not None:\n",
    "                env = Wrapper(env)\n",
    "\n",
    "            self.envs.append(env)\n",
    "\n",
    "        self.observation_space = self.envs[0].observation_space\n",
    "        self.action_space      = self.envs[0].action_space\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.envs)\n",
    "\n",
    "    def step(self, actions):\n",
    "\n",
    "        states  = []\n",
    "        rewards = []\n",
    "        dones   = [] \n",
    "        infos   = []\n",
    "\n",
    "        self.steps+= 1\n",
    "        for i in range(len(self.envs)):\n",
    "            state, reward, done, _, info = self.envs[i].step(actions[i])\n",
    "\n",
    "            if self.steps[i] > self.max_steps:\n",
    "                done = True\n",
    "\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            infos.append(info)\n",
    "\n",
    "        states  = numpy.stack(states)\n",
    "        rewards = numpy.stack(rewards)\n",
    "        dones   = numpy.stack(dones)\n",
    "\n",
    "        return states, rewards, dones, infos\n",
    "    \n",
    "    def reset_all(self):\n",
    "        states = []\n",
    "        infos  = []\n",
    "\n",
    "        for i in range(len(self.envs)):\n",
    "            state, info = self.envs[i].reset()\n",
    "           \n",
    "            states.append(state)\n",
    "            infos.append(info)\n",
    "\n",
    "        states  = numpy.stack(states)\n",
    "        self.steps[:] = 0\n",
    "\n",
    "        return states, infos\n",
    "    \n",
    "    def reset(self, env_id):\n",
    "        self.steps[env_id] = 0 \n",
    "        return self.envs[env_id].reset()\n",
    "        \n",
    "    def render(self, env_id):\n",
    "        return self.envs[env_id].render()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.envs[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating envs\n",
      "states shape  (32, 4)\n"
     ]
    }
   ],
   "source": [
    "# create multiple environments\n",
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "print(\"creating envs\")\n",
    "envs = EnvsList(env_name, 32)\n",
    "states, _ = envs.reset_all()\n",
    "\n",
    "print(\"states shape \", states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO agent\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class AgentPPO():\n",
    "    def __init__(self, envs, Model):\n",
    "        self.envs = envs\n",
    "\n",
    "        # auto select device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "        # agent hyperparameters\n",
    "        self.gamma              = 0.99\n",
    "        self.entropy_beta       = 0.001\n",
    "        self.eps_clip           = 0.1 \n",
    "        self.adv_coeff          = 1.0\n",
    "        self.val_coeff          = 0.5\n",
    "\n",
    "        self.trajectory_steps   = 128\n",
    "        self.batch_size         = 256\n",
    "        \n",
    "        self.training_epochs    = 4\n",
    "        self.envs_count         = len(envs)\n",
    "        self.learning_rate      = 0.0001\n",
    "\n",
    "\n",
    "        self.state_shape    = self.envs.observation_space.shape\n",
    "        self.actions_count  = self.envs.action_space.n\n",
    "\n",
    "        # policy buffer for storing trajectory\n",
    "        self._buffer_init()\n",
    "        \n",
    "        # create model\n",
    "        self.model = Model(self.state_shape, self.actions_count)\n",
    "        self.model.to(self.device)\n",
    "        print(self.model)\n",
    "\n",
    "        # initialise optimizer and trajectory buffer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "   \n",
    "    # agent main step\n",
    "    def step(self, states, training_enabled = False):        \n",
    "        states_t = torch.tensor(states, dtype=torch.float).to(self.device)\n",
    "\n",
    "        # obtain model output, logits and values\n",
    "        logits_t, values_t  = self.model.forward(states_t)\n",
    "\n",
    "        # sample action, probs computed from logits\n",
    "        action_probs_t        = torch.nn.functional.softmax(logits_t, dim = 1)\n",
    "        action_distribution_t = torch.distributions.Categorical(action_probs_t)\n",
    "        action_t              = action_distribution_t.sample()\n",
    "        actions               = action_t.detach().to(\"cpu\").numpy()\n",
    "       \n",
    "        # environment step\n",
    "        states_new, rewards, dones, infos = self.envs.step(actions)\n",
    "\n",
    "        #put into trajectory buffer\n",
    "        if training_enabled:\n",
    "            self._buffer_add(states_t, logits_t, values_t, actions, rewards, dones)\n",
    "\n",
    "            # if buffer is full, run training loop and clear buffer after\n",
    "            if self.buffer_ptr >= self.trajectory_steps:\n",
    "                self._compute_returns(self.gamma)\n",
    "                self._train()\n",
    "                self._buffer_init()\n",
    "  \n",
    "\n",
    "        return states_new, rewards, dones, infos\n",
    "    \n",
    "    def save(self, result_path):\n",
    "        torch.save(self.model.state_dict(), result_path + \"/model.pt\")\n",
    "\n",
    "    def load(self, result_path):\n",
    "        self.model.load_state_dict(torch.load(result_path + \"/model.pt\", map_location = self.device))\n",
    "\n",
    "    def _train(self): \n",
    "        samples_count = self.trajectory_steps*self.envs_count\n",
    "        batch_count = samples_count//self.batch_size\n",
    "\n",
    "        # epoch training\n",
    "        for e in range(self.training_epochs):\n",
    "            for batch_idx in range(batch_count):\n",
    "                # sample batch\n",
    "                states, logits, actions, returns, advantages = self._sample_batch(self.batch_size)\n",
    "                \n",
    "                # compute main PPO loss\n",
    "                loss_ppo = self.loss_ppo(states, logits, actions, returns, advantages)\n",
    "\n",
    "                self.optimizer.zero_grad()        \n",
    "                loss_ppo.backward()\n",
    "\n",
    "                # gradient clip for stabilising training\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "                self.optimizer.step() \n",
    "\n",
    "         \n",
    "\n",
    "    '''\n",
    "        main PPO loss\n",
    "    '''\n",
    "    def loss_ppo(self, states, logits, actions, returns, advantages):\n",
    "        logits_new, values_new  = self.model.forward(states)\n",
    "\n",
    "        \n",
    "        log_probs_old = torch.nn.functional.log_softmax(logits, dim = 1).detach()\n",
    "\n",
    "        probs_new     = torch.nn.functional.softmax(logits_new,     dim = 1)\n",
    "        log_probs_new = torch.nn.functional.log_softmax(logits_new, dim = 1)\n",
    "\n",
    "        '''\n",
    "            compute critic loss, as MSE\n",
    "            L = (T - V(s))^2\n",
    "        '''\n",
    "        values_new = values_new.squeeze(1)\n",
    "        loss_value = (returns.detach() - values_new)**2\n",
    "        loss_value = loss_value.mean()\n",
    "\n",
    "        ''' \n",
    "            compute actor loss, surrogate loss\n",
    "        '''\n",
    "        advantages       = self.adv_coeff*advantages.detach() \n",
    "        advantages  = (advantages - torch.mean(advantages))/(torch.std(advantages) + 1e-10)\n",
    "\n",
    "        log_probs_new_  = log_probs_new[range(len(log_probs_new)), actions]\n",
    "        log_probs_old_  = log_probs_old[range(len(log_probs_old)), actions]\n",
    "                        \n",
    "        ratio       = torch.exp(log_probs_new_ - log_probs_old_)\n",
    "        p1          = ratio*advantages\n",
    "        p2          = torch.clamp(ratio, 1.0 - self.eps_clip, 1.0 + self.eps_clip)*advantages\n",
    "        loss_policy = -torch.min(p1, p2)  \n",
    "        loss_policy = loss_policy.mean()  \n",
    "    \n",
    "        '''\n",
    "            compute entropy loss, to avoid greedy strategy\n",
    "            L = beta*H(pi(s)) = beta*pi(s)*log(pi(s))\n",
    "        '''\n",
    "        loss_entropy = (probs_new*log_probs_new).sum(dim = 1)\n",
    "        loss_entropy = self.entropy_beta*loss_entropy.mean()\n",
    "\n",
    "        loss = self.val_coeff*loss_value + loss_policy + loss_entropy\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    '''\n",
    "        trajectory buffer methods\n",
    "        this is mostly held in separated class\n",
    "    '''\n",
    "    # trajectory buffer init\n",
    "    def _buffer_init(self):\n",
    "        self.states     = torch.zeros((self.trajectory_steps, self.envs_count, ) + self.state_shape, dtype=torch.float32, device=self.device)\n",
    "        self.logits     = torch.zeros((self.trajectory_steps, self.envs_count,  self.actions_count), dtype=torch.float32, device=self.device)\n",
    "        self.values     = torch.zeros((self.trajectory_steps, self.envs_count, ), dtype=torch.float32, device=self.device)        \n",
    "        self.actions    = torch.zeros((self.trajectory_steps, self.envs_count, ), dtype=int, device=self.device)\n",
    "        self.reward     = torch.zeros((self.trajectory_steps, self.envs_count, ), dtype=torch.float32, device=self.device)\n",
    "        self.dones      = torch.zeros((self.trajectory_steps, self.envs_count, ), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.buffer_ptr = 0  \n",
    "\n",
    "    # add new items into buffer\n",
    "    def _buffer_add(self, states, logits, values, actions, rewards, dones):\n",
    "        self.states[self.buffer_ptr]    = states.detach().to(\"cpu\").clone() \n",
    "        self.logits[self.buffer_ptr]    = logits.detach().to(\"cpu\").clone() \n",
    "        self.values[self.buffer_ptr]    = values.squeeze(1).detach().to(\"cpu\").clone() \n",
    "        self.actions[self.buffer_ptr]   = torch.from_numpy(actions)\n",
    "        \n",
    "        self.reward[self.buffer_ptr]    = torch.from_numpy(rewards)\n",
    "        self.dones[self.buffer_ptr]     = torch.from_numpy(dones).float()\n",
    "        \n",
    "        self.buffer_ptr = self.buffer_ptr + 1 \n",
    "\n",
    "\n",
    "    def _compute_returns(self, gamma, lam = 0.95):\n",
    "        self.returns, self.advantages   = self._gae(self.reward, self.values, self.dones, gamma, lam)\n",
    "        \n",
    "        #reshape buffer for faster batch sampling\n",
    "        self.states     = self.states.reshape((self.trajectory_steps*self.envs_count, ) + self.state_shape)\n",
    "        self.logits     = self.logits.reshape((self.trajectory_steps*self.envs_count, self.actions_count))\n",
    "\n",
    "        self.values     = self.values.reshape((self.trajectory_steps*self.envs_count, ))        \n",
    "     \n",
    "        self.actions    = self.actions.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "        \n",
    "        self.reward     = self.reward.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "      \n",
    "        self.dones      = self.dones.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "\n",
    "        self.returns    = self.returns.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "        self.advantages = self.advantages.reshape((self.trajectory_steps*self.envs_count, ))\n",
    "   \n",
    "    # sampel random batch from buffer\n",
    "    def _sample_batch(self, batch_size):\n",
    "        indices         = torch.randint(0, self.envs_count*self.trajectory_steps, size=(batch_size, ))\n",
    "\n",
    "        states          = self.states[indices]\n",
    "        logits          = self.logits[indices]\n",
    "        \n",
    "        actions         = self.actions[indices]\n",
    "        \n",
    "        returns         = self.returns[indices]\n",
    "        advantages      = self.advantages[indices]\n",
    "       \n",
    "        return states, logits, actions, returns, advantages\n",
    "    \n",
    "    # gae returns computing - more stable than basic returns computatiom\n",
    "    def _gae(self, rewards, values, dones, gamma, lam):\n",
    "        buffer_size = rewards.shape[0]\n",
    "        envs_count  = rewards.shape[1]\n",
    "        \n",
    "        returns     = torch.zeros((buffer_size, envs_count), dtype=torch.float32, device=self.device)\n",
    "        advantages  = torch.zeros((buffer_size, envs_count), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        last_gae    = torch.zeros((envs_count), dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        for n in reversed(range(buffer_size-1)):\n",
    "            delta           = rewards[n] + gamma*values[n+1]*(1.0 - dones[n]) - values[n]\n",
    "            last_gae        = delta + gamma*lam*last_gae*(1.0 - dones[n])\n",
    "            \n",
    "            returns[n]      = last_gae + values[n]\n",
    "            advantages[n]   = last_gae\n",
    " \n",
    "        return returns, advantages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor critic model\n",
    "\n",
    "- simple fully connected model\n",
    "- 128 hidden units in two layers\n",
    "- two separated heads for actor anc critic\n",
    "\n",
    "![image](../doc/images/diagrams-fc_model.png)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    two hidden layers FC model for actor critic architecture\n",
    "'''\n",
    "class ModelFC(torch.nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, n_hidden = 128):\n",
    "        super(ModelFC, self).__init__()\n",
    "\n",
    "        n_inputs = input_shape[0]\n",
    "\n",
    "        # FC model, with two hidden layers and two output heads\n",
    "        self.lin0 = torch.nn.Linear(n_inputs, n_hidden)\n",
    "        self.act0 = torch.nn.SiLU()\n",
    "        self.lin1 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.act1 = torch.nn.SiLU()\n",
    "\n",
    "        self.lin_actor  = torch.nn.Linear(n_hidden, n_actions)\n",
    "        self.lin_critic = torch.nn.Linear(n_hidden, 1)\n",
    "\n",
    "        # orthogonal weight init\n",
    "        torch.nn.init.orthogonal_(self.lin0.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.lin0.bias)\n",
    "        torch.nn.init.orthogonal_(self.lin1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        # output layers with lower init gain\n",
    "        torch.nn.init.orthogonal_(self.lin_actor.weight, 0.01)\n",
    "        torch.nn.init.zeros_(self.lin_actor.bias)\n",
    "        torch.nn.init.orthogonal_(self.lin_critic.weight, 0.1)\n",
    "        torch.nn.init.zeros_(self.lin_critic.bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # obtain features\n",
    "        z = self.lin0(state)\n",
    "        z = self.act0(z)\n",
    "        z = self.lin1(z)\n",
    "        z = self.act1(z)\n",
    "\n",
    "        # obtain actor and critic outputs\n",
    "        logits = self.lin_actor(z)\n",
    "        value  = self.lin_critic(z)\n",
    "\n",
    "        return logits, value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelFC(\n",
      "  (lin0): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (act0): SiLU()\n",
      "  (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (act1): SiLU()\n",
      "  (lin_actor): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (lin_critic): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "0 0.0 0.0 0.0\n",
      "1000 44.78 22.219 10.656\n",
      "2000 88.97 22.625 11.033\n",
      "3000 132.88 23.469 12.664\n",
      "4000 176.91 21.156 14.864\n",
      "5000 216.09 25.562 11.388\n",
      "6000 243.16 41.594 20.909\n",
      "7000 261.97 65.375 29.59\n",
      "8000 275.62 84.625 41.943\n",
      "9000 286.88 117.688 63.776\n",
      "10000 295.41 159.875 118.398\n",
      "11000 300.44 124.656 82.391\n",
      "12000 305.62 232.719 258.25\n",
      "13000 310.59 174.5 146.416\n",
      "14000 315.47 188.438 214.131\n",
      "15000 318.28 270.312 312.035\n",
      "16000 323.97 199.969 137.683\n",
      "17000 326.97 303.812 316.283\n",
      "18000 329.34 383.781 354.817\n",
      "19000 331.88 404.969 399.948\n",
      "20000 333.44 561.188 441.126\n",
      "21000 334.72 768.188 406.984\n",
      "22000 336.0 762.5 413.389\n",
      "23000 337.28 703.406 441.81\n",
      "24000 338.69 697.438 450.375\n",
      "25000 340.0 762.25 413.936\n",
      "26000 341.34 795.094 390.123\n",
      "27000 342.53 822.906 370.964\n",
      "28000 343.66 882.781 313.02\n",
      "29000 344.75 905.906 281.211\n",
      "30000 345.97 789.094 392.149\n",
      "31000 347.22 818.188 380.613\n",
      "32000 348.62 678.438 425.381\n",
      "33000 349.72 905.0 285.621\n",
      "34000 350.94 785.469 402.939\n",
      "35000 352.16 817.406 382.216\n",
      "36000 353.25 908.312 276.166\n",
      "37000 354.38 908.438 287.789\n",
      "38000 355.5 890.875 297.163\n",
      "39000 356.62 877.719 326.172\n",
      "40000 357.66 970.156 171.731\n",
      "41000 358.78 882.438 314.089\n",
      "42000 359.94 897.906 287.812\n",
      "43000 361.09 881.438 316.484\n",
      "44000 362.22 907.906 233.756\n",
      "45000 363.34 900.594 267.911\n",
      "46000 364.44 916.375 244.79\n",
      "47000 365.47 974.438 147.894\n",
      "48000 366.53 945.375 215.842\n",
      "49000 367.66 932.906 227.332\n",
      "50000 368.72 921.0 227.186\n",
      "51000 370.16 750.0 354.927\n",
      "52000 373.66 288.219 343.911\n",
      "53000 374.84 732.75 400.777\n",
      "54000 375.84 961.5 158.063\n",
      "55000 376.84 1001.0 0.0\n",
      "56000 377.84 1001.0 0.0\n",
      "57000 378.84 1001.0 0.0\n",
      "58000 379.84 1001.0 0.0\n",
      "59000 380.84 1001.0 0.0\n",
      "60000 381.84 981.688 107.527\n",
      "61000 382.84 1001.0 0.0\n",
      "62000 383.84 1001.0 0.0\n",
      "63000 384.84 1001.0 0.0\n",
      "64000 385.88 972.719 157.463\n",
      "65000 386.88 1001.0 0.0\n",
      "66000 388.0 871.281 298.805\n",
      "67000 389.09 871.469 292.836\n",
      "68000 390.09 967.156 155.078\n",
      "69000 391.09 1001.0 0.0\n",
      "70000 392.09 1001.0 0.0\n",
      "71000 393.09 996.688 24.011\n",
      "72000 394.09 1001.0 0.0\n",
      "73000 395.09 1001.0 0.0\n",
      "74000 396.09 1001.0 0.0\n",
      "75000 397.09 1001.0 0.0\n",
      "76000 398.09 995.25 32.015\n",
      "77000 399.09 1001.0 0.0\n",
      "78000 400.12 970.469 126.69\n",
      "79000 423.75 11.062 5.0\n",
      "80000 486.56 161.969 122.621\n",
      "81000 488.5 403.562 86.618\n",
      "82000 489.5 1001.0 0.0\n",
      "83000 490.5 1001.0 0.0\n",
      "84000 491.5 1001.0 0.0\n",
      "85000 492.5 1001.0 0.0\n",
      "86000 493.5 1001.0 0.0\n",
      "87000 494.5 1001.0 0.0\n",
      "88000 495.5 1001.0 0.0\n",
      "89000 496.5 1001.0 0.0\n",
      "90000 497.5 1001.0 0.0\n",
      "91000 498.5 1001.0 0.0\n",
      "92000 499.5 1001.0 0.0\n",
      "93000 500.5 1001.0 0.0\n",
      "94000 501.5 1001.0 0.0\n",
      "95000 502.5 1001.0 0.0\n",
      "96000 503.5 1001.0 0.0\n",
      "97000 504.5 1001.0 0.0\n",
      "98000 505.53 971.156 166.163\n",
      "99000 506.53 1001.0 0.0\n",
      "\n",
      "\n",
      "\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = AgentPPO(envs, ModelFC)\n",
    "\n",
    "\n",
    "episodes_count  = numpy.zeros(len(envs))\n",
    "rewards_sum     = numpy.zeros(len(envs))\n",
    "rewards_episode = numpy.zeros(len(envs))\n",
    "\n",
    "n_steps = 100000\n",
    "\n",
    "\n",
    "states, _ = envs.reset_all()\n",
    "for n in range(n_steps):\n",
    "    # agent main step\n",
    "    states_new, rewards, dones, infos = agent.step(states, True)\n",
    "\n",
    "    # accumulate rewards for stats\n",
    "    rewards_sum+= rewards\n",
    "\n",
    "    # reset environments which finished episode \n",
    "    dones_idx = numpy.where(dones)[0]\n",
    "    for i in dones_idx:\n",
    "        states_new[i], _ = envs.reset(i)\n",
    "\n",
    "        episodes_count[i]+= 1\n",
    "        rewards_episode[i] = rewards_sum[i]\n",
    "        rewards_sum[i] = 0\n",
    "\n",
    "    states = states_new.copy()\n",
    "\n",
    "    if n%1000 == 0:\n",
    "        episodes_mean = round(episodes_count.mean(), 2)\n",
    "        rewards_mean  = round(rewards_episode.mean(), 3)\n",
    "        rewards_std   = round(rewards_episode.std(), 3)\n",
    "        print(n, episodes_mean, rewards_mean, rewards_std)\n",
    "\n",
    "\n",
    "agent.save(\"trained/CartPole/\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"training done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelFC(\n",
      "  (lin0): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (act0): SiLU()\n",
      "  (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (act1): SiLU()\n",
      "  (lin_actor): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (lin_critic): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 15:05:04.234 Python[69407:48035169] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    }
   ],
   "source": [
    "# create single env\n",
    "envs = EnvsList(env_name, 1, render_mode=\"human\")\n",
    "\n",
    "agent = AgentPPO(envs, ModelFC)\n",
    "agent.load(\"trained/CartPole/\") \n",
    "\n",
    "states, _ = envs.reset_all()\n",
    "n_steps = 1000000\n",
    "for n in range(n_steps):\n",
    "    # agent main step\n",
    "    states_new, rewards, dones, infos = agent.step(states)\n",
    "\n",
    "    # reset environments which finished episode \n",
    "    dones_idx = numpy.where(dones)[0]\n",
    "    for i in dones_idx:\n",
    "        states_new[i], _ = envs.reset(i)\n",
    "\n",
    "    states = states_new.copy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
